{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb7c246",
   "metadata": {
    "id": "3cb7c246",
    "origin_pos": 0
   },
   "source": [
    "# Incorporação de subpalavra\n",
    ":rótulo:`sec_fasttext`\n",
    "\n",
    "Em inglês,\n",
    "palavras como\n",
    "\"ajuda\", \"ajudou\" e \"ajudando\" são\n",
    "formas flexionadas da mesma palavra \"help\".\n",
    "O relacionamento\n",
    "entre \"cachorro\" e \"cachorros\"\n",
    "é o mesmo que\n",
    "que entre \"gato\" e \"gatos\",\n",
    "e\n",
    "o relacionamento\n",
    "entre \"menino\" e \"namorado\"\n",
    "é o mesmo que\n",
    "que entre \"menina\" e \"namorada\".\n",
    "Em outras línguas\n",
    "como o francês e o espanhol,\n",
    "muitos verbos têm mais de 40 formas flexionadas,\n",
    "enquanto em finlandês,\n",
    "um substantivo pode ter até 15 casos.\n",
    "Em linguística,\n",
    "A morfologia estuda a formação de palavras e as relações entre as palavras.\n",
    "No entanto,\n",
    "a estrutura interna das palavras\n",
    "não foi explorado no word2vec\n",
    "nem em GloVe.\n",
    "\n",
    "## O modelo fastText\n",
    "\n",
    "Lembre-se de como as palavras são representadas no word2vec.\n",
    "Tanto no modelo skip-gram\n",
    "e o modelo contínuo de saco de palavras,\n",
    "diferentes formas flexionadas da mesma palavra\n",
    "são representados diretamente por diferentes vetores\n",
    "sem parâmetros compartilhados.\n",
    "Para usar informações morfológicas,\n",
    "o modelo *fastText*\n",
    "propôs uma abordagem de *incorporação de subpalavras*,\n",
    "onde uma subpalavra é um caractere $n$-grama :cite:`Bojanowski.Grave.Joulin.ea.2017`.\n",
    "Em vez de aprender representações vetoriais em nível de palavra,\n",
    "fastText pode ser considerado como\n",
    "o skip-gram de nível de subpalavra,\n",
    "onde cada *palavra central* é representada pela soma de\n",
    "seus vetores de subpalavra.\n",
    "\n",
    "Vamos ilustrar como obter\n",
    "subpalavras para cada palavra central em fastText\n",
    "usando a palavra \"onde\".\n",
    "Primeiro, adicione os caracteres especiais “&lt;” e “&gt;”\n",
    "no início e no final da palavra para distinguir prefixos e sufixos de outras subpalavras.\n",
    "Em seguida, extraia os caracteres $n$-gramas da palavra.\n",
    "Por exemplo, quando $n=3$,\n",
    "obtemos todas as subpalavras de comprimento 3: \"&lt;wh\", \"whe\", \"her\", \"ere\", \"re&gt;\" e a subpalavra especial \"&lt;where&gt;\".\n",
    "\n",
    "\n",
    "No fastText, para qualquer palavra $w$,\n",
    "denotado por $\\mathcal{G}_w$\n",
    "a união de todas as suas subpalavras de comprimento entre 3 e 6\n",
    "e sua subpalavra especial.\n",
    "O vocabulário\n",
    "é a união das subpalavras de todas as palavras.\n",
    "Deixando $\\mathbf{z}_g$\n",
    "seja o vetor da subpalavra $g$ no dicionário,\n",
    "o vetor $\\mathbf{v}_w$ para\n",
    "palavra $w$ como palavra central\n",
    "no modelo skip-gram\n",
    "é a soma dos seus vetores de subpalavras:\n",
    "\n",
    "$$\\mathbf{v}_w = \\sum_{g\\in\\mathcal{G}_w} \\mathbf{z}_g.$$\n",
    "\n",
    "O resto do fastText é o mesmo que o modelo skip-gram. Comparado com o modelo skip-gram,\n",
    "o vocabulário no fastText é maior,\n",
    "resultando em mais parâmetros do modelo.\n",
    "Além do mais,\n",
    "para calcular a representação de uma palavra,\n",
    "todos os seus vetores de subpalavras\n",
    "tem que ser somado,\n",
    "levando a uma maior complexidade computacional.\n",
    "No entanto,\n",
    "graças aos parâmetros compartilhados de subpalavras entre palavras com estruturas semelhantes,\n",
    "palavras raras e até mesmo palavras fora do vocabulário\n",
    "pode obter melhores representações vetoriais no fastText.\n",
    "\n",
    "\n",
    "\n",
    "## Codificação de pares de bytes\n",
    ":label:`codificação_de_par_de_bytes_subsec`\n",
    "\n",
    "No fastText, todas as subpalavras extraídas precisam ter os comprimentos especificados, como $3$ a $6$, portanto, o tamanho do vocabulário não pode ser predefinido.\n",
    "Para permitir subpalavras de comprimento variável em um vocabulário de tamanho fixo,\n",
    "podemos aplicar um algoritmo de compressão\n",
    "chamado *codificação de pares de bytes* (BPE) para extrair subpalavras :cite:`Sennrich.Haddow.Birch.2015`.\n",
    "\n",
    "A codificação de pares de bytes realiza uma análise estatística do conjunto de dados de treinamento para descobrir símbolos comuns dentro de uma palavra,\n",
    "como caracteres consecutivos de comprimento arbitrário.\n",
    "A partir de símbolos de comprimento 1,\n",
    "A codificação de pares de bytes mescla iterativamente o par mais frequente de símbolos consecutivos para produzir novos símbolos mais longos.\n",
    "Observe que, por questões de eficiência, pares que cruzam limites de palavras não são considerados.\n",
    "No final, podemos usar esses símbolos como subpalavras para segmentar palavras.\n",
    "A codificação de pares de bytes e suas variantes têm sido usadas para representações de entrada em modelos populares de pré-treinamento de processamento de linguagem natural, como GPT-2 :cite:`Radford.Wu.Child.ea.2019` e RoBERTa :cite:`Liu.Ott.Goyal.ea.2019`.\n",
    "A seguir, ilustraremos como funciona a codificação de pares de bytes.\n",
    "\n",
    "Primeiro, inicializamos o vocabulário de símbolos como todos os caracteres minúsculos do inglês, um símbolo especial de fim de palavra `'_'` e um símbolo especial desconhecido `'[UNK]'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4071347",
   "metadata": {
    "id": "d4071347",
    "origin_pos": 1,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f3025",
   "metadata": {
    "id": "532f3025",
    "origin_pos": 2
   },
   "source": [
    "Como não consideramos pares de símbolos que cruzam os limites das palavras,\n",
    "precisamos apenas de um dicionário `raw_token_freqs` que mapeie as palavras para suas frequências (número de ocorrências)\n",
    "em um conjunto de dados.\n",
    "Observe que o símbolo especial `'_'` é anexado a cada palavra para que\n",
    "podemos recuperar facilmente uma sequência de palavras (por exemplo, \"um homem mais alto\")\n",
    "de uma sequência de símbolos de saída (por exemplo, \"um_homem_mais_alto\").\n",
    "Como iniciamos o processo de mesclagem a partir de um vocabulário de apenas caracteres únicos e símbolos especiais, um espaço é inserido entre cada par de caracteres consecutivos dentro de cada palavra (chaves do dicionário `token_freqs`).\n",
    "Em outras palavras, o espaço é o delimitador entre os símbolos dentro de uma palavra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39297a70",
   "metadata": {
    "id": "39297a70",
    "origin_pos": 3,
    "outputId": "57df1af5-7b19-4069-8b3b-04fd79db52b5",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa6f08",
   "metadata": {
    "id": "7daa6f08",
    "origin_pos": 4
   },
   "source": [
    "Definimos a seguinte função `get_max_freq_pair` que\n",
    "retorna o par mais frequente de símbolos consecutivos dentro de uma palavra,\n",
    "onde as palavras vêm das chaves do dicionário de entrada `token_freqs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17d5ac7",
   "metadata": {
    "id": "b17d5ac7",
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Key of `pairs` is a tuple of two consecutive symbols\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return max(pairs, key=pairs.get)  # Key of `pairs` with the max value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a407bb",
   "metadata": {
    "id": "c6a407bb",
    "origin_pos": 6
   },
   "source": [
    "Como uma abordagem gananciosa baseada na frequência de símbolos consecutivos,\n",
    "A codificação de pares de bytes usará a seguinte função `merge_symbols` para mesclar o par mais frequente de símbolos consecutivos para produzir novos símbolos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a9f61d",
   "metadata": {
    "id": "b2a9f61d",
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766a10d",
   "metadata": {
    "id": "e766a10d",
    "origin_pos": 8
   },
   "source": [
    "Agora, realizamos iterativamente o algoritmo de codificação de pares de bytes sobre as chaves do dicionário `token_freqs`. Na primeira iteração, o par mais frequente de símbolos consecutivos é `'t'` e `'a'`, portanto, a codificação de pares de bytes os mescla para produzir um novo símbolo `'ta'`. Na segunda iteração, a codificação de pares de bytes continua a mesclar `'ta'` e `'l'` para resultar em outro novo símbolo `'tal'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c4b2e1",
   "metadata": {
    "id": "31c4b2e1",
    "origin_pos": 9,
    "outputId": "c84ddf0d-ac0a-4032-94bc-f8dc4cecb1bc",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge #1: ('t', 'a')\n",
      "merge #2: ('ta', 'l')\n",
      "merge #3: ('tal', 'l')\n",
      "merge #4: ('f', 'a')\n",
      "merge #5: ('fa', 's')\n",
      "merge #6: ('fas', 't')\n",
      "merge #7: ('e', 'r')\n",
      "merge #8: ('er', '_')\n",
      "merge #9: ('tall', '_')\n",
      "merge #10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'merge #{i + 1}:', max_freq_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be8e54",
   "metadata": {
    "id": "33be8e54",
    "origin_pos": 10
   },
   "source": [
    "Após 10 iterações de codificação de pares de bytes, podemos ver que a lista `symbols` agora contém mais 10 símbolos que são mesclados iterativamente de outros símbolos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610ee437",
   "metadata": {
    "id": "610ee437",
    "origin_pos": 11,
    "outputId": "8a429f8f-dc23-442f-e893-a404012cc481",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c88860",
   "metadata": {
    "id": "26c88860",
    "origin_pos": 12
   },
   "source": [
    "Para o mesmo conjunto de dados especificado nas chaves do dicionário `raw_token_freqs`,\n",
    "cada palavra no conjunto de dados agora é segmentada por subpalavras \"fast_\", \"fast\", \"er_\", \"tall_\" e \"tall\"\n",
    "como resultado do algoritmo de codificação de pares de bytes.\n",
    "Por exemplo, as palavras \"faster_\" e \"taller_\" são segmentadas como \"fast er_\" e \"tall er_\", respectivamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dede4390",
   "metadata": {
    "id": "dede4390",
    "origin_pos": 13,
    "outputId": "a9a28fde-1f4c-430f-da71-73e7c2502fb8",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast_', 'fast er_', 'tall_', 'tall er_']\n"
     ]
    }
   ],
   "source": [
    "print(list(token_freqs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60999bd7",
   "metadata": {
    "id": "60999bd7",
    "origin_pos": 14
   },
   "source": [
    "Observe que o resultado da codificação de pares de bytes depende do conjunto de dados usado.\n",
    "Também podemos usar as subpalavras aprendidas de um conjunto de dados\n",
    "para segmentar palavras de outro conjunto de dados.\n",
    "Como uma abordagem gananciosa, a seguinte função `segment_BPE` tenta dividir as palavras nas maiores subpalavras possíveis a partir do argumento de entrada `symbols`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166b20c3",
   "metadata": {
    "id": "166b20c3",
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        # Segment token with the longest possible subwords from symbols\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91df344",
   "metadata": {
    "id": "e91df344",
    "origin_pos": 16
   },
   "source": [
    "A seguir, usamos as subpalavras na lista `símbolos`, que são aprendidas do conjunto de dados mencionado anteriormente,\n",
    "para segmentar `tokens` que representam outro conjunto de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0dd3cac",
   "metadata": {
    "id": "e0dd3cac",
    "origin_pos": 17,
    "outputId": "2e899198-8173-4067-d803-dc84a0ca1616",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter_']\n",
    "print(segment_BPE(tokens, symbols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee69b66",
   "metadata": {
    "id": "5ee69b66",
    "origin_pos": 18
   },
   "source": [
    "## Resumo\n",
    "\n",
    "* O modelo fastText propõe uma abordagem de incorporação de subpalavras. Baseado no modelo skip-gram em word2vec, ele representa uma palavra central como a soma de seus vetores de subpalavras.\n",
    "* A codificação de pares de bytes realiza uma análise estatística do conjunto de dados de treinamento para descobrir símbolos comuns dentro de uma palavra. Como uma abordagem gananciosa, a codificação de pares de bytes mescla iterativamente o par mais frequente de símbolos consecutivos.\n",
    "* A incorporação de subpalavras pode melhorar a qualidade das representações de palavras raras e palavras fora do dicionário.\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Como exemplo, há cerca de $3\\vezes 10^8$ $6$-gramas possíveis em inglês. Qual é o problema quando há muitas subpalavras? Como abordar o problema? Dica: consulte o final da Seção 3.2 do artigo do fastText :cite:`Bojanowski.Grave.Joulin.ea.2017`.\n",
    "1. Como projetar um modelo de incorporação de subpalavras com base no modelo de saco de palavras contínuo?\n",
    "1. Para obter um vocabulário de tamanho $m$, quantas operações de mesclagem são necessárias quando o tamanho do vocabulário de símbolos inicial é $n$?\n",
    "1. Como estender a ideia da codificação de pares de bytes para extrair frases?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a195613",
   "metadata": {
    "id": "8a195613",
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussões](https://discuss.d2l.ai/t/4587)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
