{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1520b653",
   "metadata": {
    "id": "1520b653"
   },
   "source": [
    "As seguintes bibliotecas adicionais são necessárias para executar este\n",
    "notebook. Observe que a execução no Colab é experimental, por favor, relate um Github\n",
    "questão se você tiver algum problema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f5f41",
   "metadata": {
    "id": "a56f5f41"
   },
   "outputs": [],
   "source": [
    "!pip install d2l==1.0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2043d21",
   "metadata": {
    "id": "d2043d21",
    "origin_pos": 0
   },
   "source": [
    "# Representações de codificadores bidirecionais de transformadores (BERT)\n",
    ":rótulo:`sec_bert`\n",
    "\n",
    "Introduzimos vários modelos de incorporação de palavras para compreensão da linguagem natural.\n",
    "Após o pré-treinamento, a saída pode ser considerada uma matriz\n",
    "onde cada linha é um vetor que representa uma palavra de um vocabulário predefinido.\n",
    "Na verdade, todos esses modelos de incorporação de palavras são *independentes do contexto*.\n",
    "Vamos começar ilustrando essa propriedade.\n",
    "\n",
    "\n",
    "## De independente de contexto para sensível ao contexto\n",
    "\n",
    "Lembre-se dos experimentos em :numref:`sec_word2vec_pretraining` e :numref:`sec_synonyms`.\n",
    "Por exemplo, word2vec e GloVe atribuem o mesmo vetor pré-treinado à mesma palavra, independentemente do contexto da palavra (se houver).\n",
    "Formalmente, uma representação independente de contexto de qualquer token $x$\n",
    "é uma função $f(x)$ que recebe apenas $x$ como entrada.\n",
    "Dada a abundância de polissemia e semântica complexa nas línguas naturais,\n",
    "representações independentes de contexto têm limitações óbvias.\n",
    "Por exemplo, a palavra \"guindaste\" em contextos\n",
    "\"um guindaste está voando\" e \"um motorista de guindaste veio\" têm significados completamente diferentes;\n",
    "assim, a mesma palavra pode receber representações diferentes dependendo do contexto.\n",
    "\n",
    "Isso motiva o desenvolvimento de representações de palavras *sensíveis ao contexto*,\n",
    "onde as representações de palavras dependem de seus contextos.\n",
    "Portanto, uma representação sensível ao contexto do token $x$ é uma função $f(x, c(x))$\n",
    "dependendo tanto de $x$ quanto de seu contexto $c(x)$.\n",
    "Representações populares sensíveis ao contexto\n",
    "incluir TagLM (marcador de sequência aumentada por modelo de linguagem) :cite:`Peters.Ammar.Bhagavatula.ea.2017`,\n",
    "CoVe (Vetores de Contexto) :cite:`McCann.Bradbury.Xiong.ea.2017`,\n",
    "e ELMo (Embeddings de Modelos de Linguagem):cite:`Peters.Neumann.Iyyer.ea.2018`.\n",
    "\n",
    "Por exemplo, tomando a sequência inteira como entrada,\n",
    "ELMo é uma função que atribui uma representação a cada palavra da sequência de entrada.\n",
    "Especificamente, o ELMo combina todas as representações da camada intermediária do LSTM bidirecional pré-treinado como a representação de saída.\n",
    "Em seguida, a representação ELMo será adicionada ao modelo supervisionado existente de uma tarefa posterior\n",
    "como recursos adicionais, como concatenar a representação ELMo e a representação original (por exemplo, GloVe) de tokens no modelo existente.\n",
    "Por um lado,\n",
    "todos os pesos no modelo LSTM bidirecional pré-treinado são congelados após as representações ELMo serem adicionadas.\n",
    "Por outro lado,\n",
    "o modelo supervisionado existente é especificamente personalizado para uma determinada tarefa.\n",
    "Aproveitando diferentes modelos para diferentes tarefas naquele momento,\n",
    "a adição do ELMo melhorou o estado da arte em seis tarefas de processamento de linguagem natural:\n",
    "análise de sentimentos, inferência de linguagem natural,\n",
    "rotulagem de papéis semânticos, resolução de correferências,\n",
    "reconhecimento de entidade nomeada e resposta a perguntas.\n",
    "\n",
    "\n",
    "## De específico para tarefa para agnóstico para tarefa\n",
    "\n",
    "Embora o ELMo tenha melhorado significativamente as soluções para um conjunto diversificado de tarefas de processamento de linguagem natural,\n",
    "cada solução ainda depende de uma arquitetura *específica da tarefa*.\n",
    "No entanto, é praticamente não trivial criar uma arquitetura específica para cada tarefa de processamento de linguagem natural.\n",
    "O modelo GPT (Generative Pre-Training) representa um esforço na concepção\n",
    "um modelo geral *independente de tarefas* para representações sensíveis ao contexto :cite:`Radford.Narasimhan.Salimans.ea.2018`.\n",
    "Construído em um decodificador Transformer,\n",
    "O GPT pré-treina um modelo de linguagem que será usado para representar sequências de texto.\n",
    "Ao aplicar GPT a uma tarefa downstream,\n",
    "a saída do modelo de linguagem será alimentada em uma camada de saída linear adicionada\n",
    "para prever o rótulo da tarefa.\n",
    "Em nítido contraste com o ELMo que congela os parâmetros do modelo pré-treinado,\n",
    "O GPT ajusta *todos* os parâmetros no decodificador Transformer pré-treinado\n",
    "durante o aprendizado supervisionado da tarefa subsequente.\n",
    "O GPT foi avaliado em doze tarefas de inferência de linguagem natural,\n",
    "resposta a perguntas, semelhança de frases e classificação,\n",
    "e melhorou o estado da arte em nove deles com mudanças mínimas\n",
    "para a arquitetura do modelo.\n",
    "\n",
    "No entanto, devido à natureza autorregressiva dos modelos de linguagem,\n",
    "O GPT olha apenas para a frente (da esquerda para a direita).\n",
    "Nos contextos \"fui ao banco para depositar dinheiro\" e \"fui ao banco para me sentar\",\n",
    "como \"banco\" é sensível ao contexto à sua esquerda,\n",
    "O GPT retornará a mesma representação para \"banco\",\n",
    "embora tenha significados diferentes.\n",
    "\n",
    "\n",
    "## BERT: Combinando o melhor dos dois mundos\n",
    "\n",
    "Como vimos,\n",
    "O ELMo codifica o contexto bidirecionalmente, mas usa arquiteturas específicas para tarefas;\n",
    "enquanto o GPT é independente de tarefa, mas codifica o contexto da esquerda para a direita.\n",
    "Combinando o melhor dos dois mundos,\n",
    "BERT (Representações de codificadores bidirecionais de transformadores)\n",
    "codifica o contexto bidirecionalmente e requer mudanças mínimas na arquitetura\n",
    "para uma ampla gama de tarefas de processamento de linguagem natural :cite:`Devlin.Chang.Lee.ea.2018`.\n",
    "Usando um codificador Transformer pré-treinado,\n",
    "BERT é capaz de representar qualquer token com base em seu contexto bidirecional.\n",
    "Durante a aprendizagem supervisionada de tarefas posteriores,\n",
    "O BERT é semelhante ao GPT em dois aspectos.\n",
    "Primeiro, as representações BERT serão alimentadas em uma camada de saída adicional,\n",
    "com alterações mínimas na arquitetura do modelo dependendo da natureza das tarefas,\n",
    "como prever para cada token vs. prever para a sequência inteira.\n",
    "Segundo,\n",
    "todos os parâmetros do codificador Transformer pré-treinado são ajustados com precisão,\n",
    "enquanto a camada de saída adicional será treinada do zero.\n",
    ":numref:`fig_elmo-gpt-bert` descreve as diferenças entre ELMo, GPT e BERT.\n",
    "\n",
    "![Uma comparação de ELMo, GPT e BERT.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/elmo-gpt-bert.svg?raw=1)\n",
    ":label:`fig_elmo-gpt-bert`\n",
    "\n",
    "\n",
    "O BERT melhorou ainda mais o estado da arte em onze tarefas de processamento de linguagem natural\n",
    "sob categorias amplas de (i) classificação de texto único (por exemplo, análise de sentimentos), (ii) classificação de pares de textos (por exemplo, inferência de linguagem natural),\n",
    "(iii) resposta a perguntas, (iv) marcação de texto (por exemplo, reconhecimento de entidade nomeada).\n",
    "Tudo proposto em 2018,\n",
    "de ELMo sensível ao contexto para GPT e BERT independentes de tarefas,\n",
    "O pré-treinamento conceitualmente simples, mas empiricamente poderoso, de representações profundas para linguagens naturais revolucionou soluções para diversas tarefas de processamento de linguagem natural.\n",
    "\n",
    "No restante deste capítulo,\n",
    "vamos nos aprofundar no pré-treinamento do BERT.\n",
    "Quando as aplicações de processamento de linguagem natural são explicadas em :numref:`chap_nlp_app`,\n",
    "ilustraremos o ajuste fino do BERT para aplicações posteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa5c8df",
   "metadata": {
    "id": "ffa5c8df",
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86eb35f",
   "metadata": {
    "id": "c86eb35f",
    "origin_pos": 3
   },
   "source": [
    "## [**Representação de entrada**]\n",
    ":rótulo:`subsec_bert_input_rep`\n",
    "\n",
    "No processamento de linguagem natural,\n",
    "algumas tarefas (por exemplo, análise de sentimentos) usam um único texto como entrada,\n",
    "enquanto em algumas outras tarefas (por exemplo, inferência de linguagem natural),\n",
    "a entrada é um par de sequências de texto.\n",
    "A sequência de entrada BERT representa de forma inequívoca tanto texto único quanto pares de texto.\n",
    "No primeiro,\n",
    "a sequência de entrada BERT é a concatenação de\n",
    "o token de classificação especial “&lt;cls&gt;”,\n",
    "tokens de uma sequência de texto,\n",
    "e o token de separação especial “&lt;sep&gt;”.\n",
    "Neste último,\n",
    "a sequência de entrada BERT é a concatenação de\n",
    "“&lt;cls&gt;”, tokens da primeira sequência de texto,\n",
    "“&lt;sep&gt;”, tokens da segunda sequência de texto, e “&lt;sep&gt;”.\n",
    "Distinguiremos consistentemente a terminologia \"sequência de entrada BERT\"\n",
    "de outros tipos de \"sequências\".\n",
    "Por exemplo, uma *sequência de entrada BERT* pode incluir uma *sequência de texto* ou duas *sequências de texto*.\n",
    "\n",
    "Para distinguir pares de texto,\n",
    "os embeddings de segmento aprendidos $\\mathbf{e}_A$ e $\\mathbf{e}_B$\n",
    "são adicionados aos embeddings de token da primeira sequência e da segunda sequência, respectivamente.\n",
    "Para entradas de texto simples, somente $\\mathbf{e}_A$ é usado.\n",
    "\n",
    "O seguinte `get_tokens_and_segments` aceita uma ou duas frases\n",
    "como entrada, então retorna tokens da sequência de entrada BERT\n",
    "e seus IDs de segmento correspondentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c018a43",
   "metadata": {
    "id": "3c018a43",
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cea11",
   "metadata": {
    "id": "948cea11",
    "origin_pos": 5
   },
   "source": [
    "O BERT escolhe o codificador Transformer como sua arquitetura bidirecional.\n",
    "Comum no codificador do transformador,\n",
    "embeddings posicionais são adicionados em cada posição da sequência de entrada BERT.\n",
    "Entretanto, diferente do codificador Transformer original,\n",
    "O BERT usa embeddings posicionais *aprendíveis*.\n",
    "Para resumir, :numref:`fig_bert-input` mostra que\n",
    "as incorporações da sequência de entrada BERT são a soma\n",
    "dos embeddings de token, embeddings de segmento e embeddings posicionais.\n",
    "\n",
    "![Os embeddings da sequência de entrada BERT são a soma\n",
    "dos embeddings de token, embeddings de segmento e embeddings posicionais.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/bert-input.svg?raw=1)\n",
    ":label:`fig_bert-entrada`\n",
    "\n",
    "A seguinte [**`classe BERTEncoder`**] é semelhante à classe `TransformerEncoder`\n",
    "conforme implementado em :numref:`sec_transformer`.\n",
    "Diferente de `TransformerEncoder`, `BERTEncoder` usa\n",
    "embeddings de segmento e embeddings posicionais aprendíveis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a90baa53",
   "metadata": {
    "id": "a90baa53",
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout, max_len=1000, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", d2l.TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # In BERT, positional embeddings are learnable, thus we create a\n",
    "        # parameter of positional embeddings that are long enough\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99ce42",
   "metadata": {
    "id": "ab99ce42",
    "origin_pos": 8
   },
   "source": [
    "Suponha que o tamanho do vocabulário seja 10000.\n",
    "Para demonstrar a [**inferência de `BERTEncoder`**] para a frente,\n",
    "vamos criar uma instância dele e inicializar seus parâmetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eb45fe4",
   "metadata": {
    "id": "7eb45fe4",
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "ffn_num_input, num_blks, dropout = 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                      num_blks, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52eb2f",
   "metadata": {
    "id": "7c52eb2f",
    "origin_pos": 11
   },
   "source": [
    "Definimos `tokens` como 2 sequências de entrada BERT de comprimento 8,\n",
    "onde cada token é um índice do vocabulário.\n",
    "A inferência direta de `BERTEncoder` com a entrada `tokens`\n",
    "retorna o resultado codificado onde cada token é representado por um vetor\n",
    "cujo comprimento é predefinido pelo hiperparâmetro `num_hiddens`.\n",
    "Este hiperparâmetro é geralmente chamado de *tamanho oculto*\n",
    "(número de unidades ocultas) do codificador do transformador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56903d71",
   "metadata": {
    "id": "56903d71",
    "origin_pos": 13,
    "outputId": "eab96a01-3086-41ed-a41f-d073df76e9aa",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09881b5d",
   "metadata": {
    "id": "09881b5d",
    "origin_pos": 14
   },
   "source": [
    "## Tarefas de pré-treinamento\n",
    ":label:`tarefas_de_pré-treinamento_subsec_bert`\n",
    "\n",
    "A inferência direta de `BERTEncoder` fornece a representação BERT\n",
    "de cada token do texto de entrada e do inserido\n",
    "tokens especiais “&lt;cls&gt;” e “&lt;seq&gt;”.\n",
    "A seguir, usaremos essas representações para calcular a função de perda\n",
    "para pré-treinamento BERT.\n",
    "O pré-treinamento é composto pelas duas tarefas seguintes:\n",
    "modelagem de linguagem mascarada e previsão da próxima frase.\n",
    "\n",
    "### [**Modelagem de Linguagem Mascarada**]\n",
    ":rótulo:`subsec_mlm`\n",
    "\n",
    "Conforme ilustrado em :numref:`sec_language-model`,\n",
    "um modelo de linguagem prevê um token usando o contexto à sua esquerda.\n",
    "Para codificar o contexto bidirecionalmente para representar cada token,\n",
    "BERT mascara tokens aleatoriamente e usa tokens do contexto bidirecional para\n",
    "prever os tokens mascarados de forma autossupervisionada.\n",
    "Essa tarefa é chamada de *modelo de linguagem mascarada*.\n",
    "\n",
    "Nesta tarefa de pré-treinamento,\n",
    "15% dos tokens serão selecionados aleatoriamente como tokens mascarados para previsão.\n",
    "Para prever um token mascarado sem trapacear usando o rótulo,\n",
    "uma abordagem direta é sempre substituí-lo por um token especial “&lt;mask&gt;” na sequência de entrada BERT.\n",
    "No entanto, o token especial artificial “&lt;mask&gt;” nunca aparecerá\n",
    "em ajuste fino.\n",
    "Para evitar tal incompatibilidade entre o pré-treinamento e o ajuste fino,\n",
    "se um token for mascarado para previsão (por exemplo, \"ótimo\" é selecionado para ser mascarado e previsto em \"este filme é ótimo\"),\n",
    "na entrada será substituído por:\n",
    "\n",
    "* um token especial “&lt;mask&gt;” para 80% do tempo (por exemplo, \"este filme é ótimo\" se torna \"este filme é &lt;mask&gt;\");\n",
    "* um token aleatório para 10% do tempo (por exemplo, \"este filme é ótimo\" se torna \"este filme é uma bebida\");\n",
    "* o token de rótulo inalterado por 10% do tempo (por exemplo, \"este filme é ótimo\" se torna \"este filme é ótimo\").\n",
    "\n",
    "Observe que durante 10% de 15% do tempo um token aleatório é inserido.\n",
    "Esse ruído ocasional incentiva o BERT a ser menos tendencioso em relação ao token mascarado (especialmente quando o token de rótulo permanece inalterado) em sua codificação de contexto bidirecional.\n",
    "\n",
    "Implementamos a seguinte classe `MaskLM` para prever tokens mascarados\n",
    "na tarefa do modelo de linguagem mascarada do pré-treinamento de BERT.\n",
    "A previsão usa um MLP de camada oculta (`self.mlp`).\n",
    "Na inferência direta, são necessárias duas entradas:\n",
    "o resultado codificado de `BERTEncoder` e as posições do token para previsão.\n",
    "A saída são os resultados da previsão nessas posições.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8614e46",
   "metadata": {
    "id": "a8614e46",
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class MaskLM(nn.Module):\n",
    "    \"\"\"The masked language model task of BERT.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.LazyLinear(vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n",
    "        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1f5c9",
   "metadata": {
    "id": "bce1f5c9",
    "origin_pos": 17
   },
   "source": [
    "Para demonstrar [**a inferência direta de `MaskLM`**],\n",
    "criamos sua instância `mlm` e a inicializamos.\n",
    "Lembre-se de `encoded_X` da inferência direta de `BERTEncoder`\n",
    "representa 2 sequências de entrada BERT.\n",
    "Definimos `mlm_positions` como os 3 índices a serem previstos em qualquer sequência de entrada BERT de `encoded_X`.\n",
    "A inferência direta de `mlm` retorna resultados de previsão `mlm_Y_hat`\n",
    "em todas as posições mascaradas `mlm_positions` de `encoded_X`.\n",
    "Para cada previsão, o tamanho do resultado é igual ao tamanho do vocabulário.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b3fc7d6",
   "metadata": {
    "id": "6b3fc7d6",
    "origin_pos": 19,
    "outputId": "12f1fa20-c9ca-4c3c-8115-88121c31bafb",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd88359",
   "metadata": {
    "id": "5cd88359",
    "origin_pos": 20
   },
   "source": [
    "Com os rótulos de verdade básica `mlm_Y` dos tokens previstos `mlm_Y_hat` sob máscaras,\n",
    "podemos calcular a perda de entropia cruzada da tarefa do modelo de linguagem mascarada no pré-treinamento de BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d85768b",
   "metadata": {
    "id": "8d85768b",
    "origin_pos": 22,
    "outputId": "879abaee-dcca-421d-814f-a7111991db9c",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b8c02",
   "metadata": {
    "id": "c78b8c02",
    "origin_pos": 23
   },
   "source": [
    "### [**Previsão da próxima frase**]\n",
    ":rótulo:`subsec_nsp`\n",
    "\n",
    "Embora a modelagem de linguagem mascarada seja capaz de codificar contexto bidirecional\n",
    "para representar palavras, não modela explicitamente a relação lógica\n",
    "entre pares de texto.\n",
    "Para ajudar a entender a relação entre duas sequências de texto,\n",
    "O BERT considera uma tarefa de classificação binária, *previsão da próxima frase*, em seu pré-treinamento.\n",
    "Ao gerar pares de frases para pré-treinamento,\n",
    "na metade do tempo são de fato frases consecutivas com o rótulo \"Verdadeiro\";\n",
    "enquanto na outra metade do tempo a segunda frase é amostrada aleatoriamente do corpus com o rótulo \"Falso\".\n",
    "\n",
    "A seguinte classe `NextSentencePred` usa um MLP de camada oculta\n",
    "para prever se a segunda frase é a próxima frase da primeira\n",
    "na sequência de entrada do BERT.\n",
    "Devido à autoatenção no codificador do transformador,\n",
    "a representação BERT do token especial “&lt;cls&gt;”\n",
    "codifica as duas frases da entrada.\n",
    "Portanto, a camada de saída (`self.output`) do classificador MLP recebe `X` como entrada,\n",
    "onde `X` é a saída da camada oculta MLP cuja entrada é o token codificado “&lt;cls&gt;”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1951876",
   "metadata": {
    "id": "c1951876",
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.LazyLinear(2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X` shape: (batch size, `num_hiddens`)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9f8eb",
   "metadata": {
    "id": "52f9f8eb",
    "origin_pos": 26
   },
   "source": [
    "Podemos ver que [**a inferência direta de uma instância `NextSentencePred`**]\n",
    "retorna previsões binárias para cada sequência de entrada BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aba0cce5",
   "metadata": {
    "id": "aba0cce5",
    "origin_pos": 28,
    "outputId": "dced7c07-b5d4-4180-a37d-e32d4a069da4",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch by default will not flatten the tensor as seen in mxnet where, if\n",
    "# flatten=True, all but the first axis of input data are collapsed together\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "# input_shape for NSP: (batch size, `num_hiddens`)\n",
    "nsp = NextSentencePred()\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacce83",
   "metadata": {
    "id": "6aacce83",
    "origin_pos": 29
   },
   "source": [
    "A perda de entropia cruzada das duas classificações binárias também pode ser calculada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba504299",
   "metadata": {
    "id": "ba504299",
    "origin_pos": 31,
    "outputId": "9bba9da7-d16a-4279-cf1c-42e0b3183dc1",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac0df2",
   "metadata": {
    "id": "21ac0df2",
    "origin_pos": 32
   },
   "source": [
    "É digno de nota que todos os rótulos em ambas as tarefas de pré-treinamento acima mencionadas\n",
    "pode ser obtido trivialmente do corpus de pré-treinamento sem esforço de rotulagem manual.\n",
    "O BERT original foi pré-treinado na concatenação do BookCorpus :cite:`Zhu.Kiros.Zemel.ea.2015`\n",
    "e Wikipédia em inglês.\n",
    "Esses dois corpora de texto são enormes:\n",
    "eles têm 800 milhões de palavras e 2,5 bilhões de palavras, respectivamente.\n",
    "\n",
    "\n",
    "## [**Juntando tudo**]\n",
    "\n",
    "Ao pré-treinar o BERT, a função de perda final é uma combinação linear de\n",
    "ambas as funções de perda para modelagem de linguagem mascarada e previsão da próxima frase.\n",
    "Agora podemos definir a classe `BERTModel` instanciando as três classes\n",
    "`BERTEncoder`, `MaskLM` e `NextSentencePred`.\n",
    "A inferência direta retorna as representações BERT codificadas `encoded_X`,\n",
    "previsões de modelagem de linguagem mascarada `mlm_Y_hat`,\n",
    "e as próximas previsões de frases `nsp_Y_hat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73c331cd",
   "metadata": {
    "id": "73c331cd",
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"The BERT model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_blks, dropout, max_len=1000):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                                   num_heads, num_blks, dropout,\n",
    "                                   max_len=max_len)\n",
    "        self.hidden = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
    "        self.nsp = NextSentencePred()\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # The hidden layer of the MLP classifier for next sentence prediction.\n",
    "        # 0 is the index of the '<cls>' token\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55112a",
   "metadata": {
    "id": "be55112a",
    "origin_pos": 35
   },
   "source": [
    "## Resumo\n",
    "\n",
    "* Modelos de incorporação de palavras como word2vec e GloVe são independentes de contexto. Eles atribuem o mesmo vetor pré-treinado à mesma palavra, independentemente do contexto da palavra (se houver). É difícil para eles lidar bem com polissemia ou semântica complexa em línguas naturais.\n",
    "* Para representações de palavras sensíveis ao contexto, como ELMo e GPT, as representações de palavras dependem de seus contextos.\n",
    "* O ELMo codifica o contexto bidirecionalmente, mas usa arquiteturas específicas para cada tarefa (no entanto, é praticamente não trivial criar uma arquitetura específica para cada tarefa de processamento de linguagem natural); enquanto o GPT é independente de tarefa, mas codifica o contexto da esquerda para a direita.\n",
    "* O BERT combina o melhor dos dois mundos: ele codifica o contexto bidirecionalmente e requer mudanças mínimas de arquitetura para uma ampla gama de tarefas de processamento de linguagem natural.\n",
    "* Os embeddings da sequência de entrada BERT são a soma dos embeddings de tokens, embeddings de segmentos e embeddings posicionais.\n",
    "* O pré-treinamento BERT é composto de duas tarefas: modelagem de linguagem mascarada e previsão da próxima frase. A primeira é capaz de codificar contexto bidirecional para representar palavras, enquanto a última modela explicitamente o relacionamento lógico entre pares de texto.\n",
    "\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Todas as outras coisas sendo iguais, um modelo de linguagem mascarada exigirá mais ou menos etapas de pré-treinamento para convergir do que um modelo de linguagem da esquerda para a direita? Por quê?\n",
    "1. Na implementação original do BERT, a rede de feed-forward posicional em `BERTEncoder` (via `d2l.TransformerEncoderBlock`) e a camada totalmente conectada em `MaskLM` usam a unidade linear de erro gaussiano (GELU) :cite:`Hendrycks.Gimpel.2016` como a função de ativação. Pesquise sobre a diferença entre GELU e ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727cff7",
   "metadata": {
    "id": "9727cff7",
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussões](https://discuss.d2l.ai/t/1490)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
