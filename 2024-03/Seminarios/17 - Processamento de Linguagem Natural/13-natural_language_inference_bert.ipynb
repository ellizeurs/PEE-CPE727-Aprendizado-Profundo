{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bac560a",
   "metadata": {
    "id": "6bac560a"
   },
   "source": [
    "As seguintes bibliotecas adicionais são necessárias para executar este\n",
    "notebook. Observe que a execução no Colab é experimental, por favor, relate um Github\n",
    "questão se você tiver algum problema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e3829",
   "metadata": {
    "id": "c34e3829"
   },
   "outputs": [],
   "source": [
    "!pip install d2l==1.0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f38c1",
   "metadata": {
    "id": "f25f38c1",
    "origin_pos": 0
   },
   "source": [
    "# Inferência de linguagem natural: ajuste fino de BERT\n",
    ":label:`sec_natural-language-inference-bert`\n",
    "\n",
    "Nas seções anteriores deste capítulo,\n",
    "nós projetamos uma arquitetura baseada na atenção\n",
    "(em :numref:`sec_natural-language-inference-attention`)\n",
    "para a tarefa de inferência de linguagem natural\n",
    "no conjunto de dados SNLI (conforme descrito em :numref:`sec_natural-language-inference-and-dataset`).\n",
    "Agora revisitamos essa tarefa ajustando o BERT.\n",
    "Conforme discutido em :numref:`sec_finetuning-bert`,\n",
    "a inferência em linguagem natural é um problema de classificação de pares de texto em nível de sequência,\n",
    "e o ajuste fino do BERT requer apenas uma arquitetura adicional baseada em MLP,\n",
    "conforme ilustrado em :numref:`fig_nlp-map-nli-bert`.\n",
    "\n",
    "![Esta seção alimenta BERT pré-treinado para uma arquitetura baseada em MLP para inferência de linguagem natural.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/nlp-map-nli-bert.svg?raw=1)\n",
    ":label:`fig_nlp-map-nli-bert`\n",
    "\n",
    "Nesta seção,\n",
    "faremos o download de uma versão pequena pré-treinada do BERT,\n",
    "então ajuste-o\n",
    "para inferência de linguagem natural no conjunto de dados SNLI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f088de5",
   "metadata": {
    "id": "9f088de5",
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf2d0d",
   "metadata": {
    "id": "caaf2d0d",
    "origin_pos": 3
   },
   "source": [
    "## [**Carregando BERT pré-treinado**]\n",
    "\n",
    "Explicamos como pré-treinar BERT no conjunto de dados WikiText-2 em\n",
    ":numref:`sec_bert-dataset` e :numref:`sec_bert-pretraining`\n",
    "(observe que o modelo BERT original é pré-treinado em corpora muito maiores).\n",
    "Conforme discutido em :numref:`sec_bert-pretraining`,\n",
    "o modelo BERT original tem centenas de milhões de parâmetros.\n",
    "A seguir,\n",
    "Nós fornecemos duas versões de BERT pré-treinado:\n",
    "\"bert.base\" é quase tão grande quanto o modelo base BERT original que requer muitos recursos computacionais para ajuste fino,\n",
    "enquanto \"bert.small\" é uma versão pequena para facilitar a demonstração.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd9ca6e",
   "metadata": {
    "id": "fdd9ca6e",
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e761acc",
   "metadata": {
    "id": "6e761acc",
    "origin_pos": 6
   },
   "source": [
    "Qualquer modelo BERT pré-treinado contém um arquivo \"vocab.json\" que define o conjunto de vocabulário\n",
    "e um arquivo \"pretrained.params\" dos parâmetros pré-treinados.\n",
    "Implementamos a seguinte função `load_pretrained_model` para [**carregar parâmetros BERT pré-treinados**].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca530a2",
   "metadata": {
    "id": "9ca530a2",
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_blks, dropout, max_len, devices):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # Define an empty vocabulary to load the predefined vocabulary\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    bert = d2l.BERTModel(\n",
    "        len(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=4,\n",
    "        num_blks=2, dropout=0.2, max_len=max_len)\n",
    "    # Load pretrained BERT parameters\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527319d5",
   "metadata": {
    "id": "527319d5",
    "origin_pos": 9
   },
   "source": [
    "Para facilitar a demonstração na maioria das máquinas,\n",
    "Nesta seção, carregaremos e ajustaremos a versão pequena (\"bert.small\") do BERT pré-treinado.\n",
    "No exercício, mostraremos como ajustar o \"bert.base\" muito maior para melhorar significativamente a precisão dos testes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d73006",
   "metadata": {
    "id": "b4d73006",
    "origin_pos": 10,
    "outputId": "6d7e6233-6066-4cc5-f536-3002ea04767a",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/bert.small.torch.zip from http://d2l-data.s3-accelerate.amazonaws.com/bert.small.torch.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196239/325226183.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert.load_state_dict(torch.load(os.path.join(data_dir,\n"
     ]
    }
   ],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
    "    num_blks=2, dropout=0.1, max_len=512, devices=devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599a861",
   "metadata": {
    "id": "9599a861",
    "origin_pos": 11
   },
   "source": [
    "## [**O conjunto de dados para ajuste fino do BERT**]\n",
    "\n",
    "Para a tarefa de downstream inferência de linguagem natural no conjunto de dados SNLI,\n",
    "definimos uma classe de conjunto de dados personalizada `SNLIBERTDataset`.\n",
    "Em cada exemplo,\n",
    "a premissa e a hipótese formam um par de sequências de texto\n",
    "e é compactado em uma sequência de entrada BERT, conforme descrito em :numref:`fig_bert-two-seqs`.\n",
    "Lembre-se de :numref:`subsec_bert_input_rep` que os IDs de segmento\n",
    "são usados ​​para distinguir a premissa e a hipótese em uma sequência de entrada BERT.\n",
    "Com o comprimento máximo predefinido de uma sequência de entrada BERT (`max_len`),\n",
    "o último token do par de texto de entrada mais longo continua sendo removido até\n",
    "`max_len` é atendido.\n",
    "Para acelerar a geração do conjunto de dados SNLI\n",
    "para ajuste fino do BERT,\n",
    "Usamos 4 processos de trabalho para gerar exemplos de treinamento ou teste em paralelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef1ad4c",
   "metadata": {
    "id": "0ef1ad4c",
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n",
    "        # input\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e4031",
   "metadata": {
    "id": "972e4031",
    "origin_pos": 14
   },
   "source": [
    "Após baixar o conjunto de dados SNLI,\n",
    "nós [**geramos exemplos de treinamento e teste**]\n",
    "instanciando a classe `SNLIBERTDataset`.\n",
    "Esses exemplos serão lidos em minibatches durante o treinamento e os testes\n",
    "de inferência de linguagem natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8fa6e9",
   "metadata": {
    "id": "ba8fa6e9",
    "origin_pos": 16,
    "outputId": "3a2db659-d45c-4b58-b503-5953fa6c89e5",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "# Reduce `batch_size` if there is an out of memory error. In the original BERT\n",
    "# model, `max_len` = 512\n",
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                   num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                  num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072ac02",
   "metadata": {
    "id": "1072ac02",
    "origin_pos": 17
   },
   "source": [
    "## Ajuste fino do BERT\n",
    "\n",
    "Como :numref:`fig_bert-two-seqs` indica,\n",
    "ajuste fino de BERT para inferência de linguagem natural\n",
    "requer apenas um MLP extra consistindo de duas camadas totalmente conectadas\n",
    "(veja `self.hidden` e `self.output` na seguinte classe `BERTClassifier`).\n",
    "[**Este MLP transforma o\n",
    "Representação BERT do token especial “&lt;cls&gt;”**],\n",
    "que codifica a informação tanto da premissa quanto da hipótese,\n",
    "(**em três saídas de inferência de linguagem natural**):\n",
    "implicação, contradição e neutralidade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b8d66",
   "metadata": {
    "id": "a65b8d66",
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.LazyLinear(3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a14374",
   "metadata": {
    "id": "02a14374",
    "origin_pos": 20
   },
   "source": [
    "A seguir,\n",
    "o modelo BERT pré-treinado `bert` é alimentado na instância `BERTClassifier` `net` para\n",
    "a aplicação downstream.\n",
    "Em implementações comuns de ajuste fino de BERT,\n",
    "somente os parâmetros da camada de saída do MLP adicional (`net.output`) serão aprendidos do zero.\n",
    "Todos os parâmetros do codificador BERT pré-treinado (`net.encoder`) e da camada oculta do MLP adicional (`net.hidden`) serão ajustados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a26a4",
   "metadata": {
    "id": "b92a26a4",
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = BERTClassifier(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b303e5c",
   "metadata": {
    "id": "1b303e5c",
    "origin_pos": 23
   },
   "source": [
    "Lembre-se de que\n",
    "em :numref:`sec_bert`\n",
    "tanto a classe `MaskLM` quanto a classe `NextSentencePred`\n",
    "têm parâmetros em seus MLPs empregados.\n",
    "Esses parâmetros fazem parte daqueles do modelo BERT pré-treinado\n",
    "`bert` e, portanto, parte dos parâmetros em `net`.\n",
    "No entanto, tais parâmetros são apenas para computação\n",
    "a perda da modelagem da linguagem mascarada\n",
    "e a próxima frase previsão perda\n",
    "durante o pré-treinamento.\n",
    "Essas duas funções de perda são irrelevantes para o ajuste fino de aplicações downstream,\n",
    "assim os parâmetros dos MLPs empregados em\n",
    "`MaskLM` e `NextSentencePred` não são atualizados (obsoletos) quando o BERT é ajustado.\n",
    "\n",
    "Para permitir parâmetros com gradientes obsoletos,\n",
    "o sinalizador `ignore_stale_grad=True` é definido na função `step` de `d2l.train_batch_ch13`.\n",
    "Usamos esta função para treinar e avaliar o modelo `net` usando o conjunto de treinamento\n",
    "(`train_iter`) e o conjunto de testes (`test_iter`) do SNLI.\n",
    "Devido aos recursos computacionais limitados, [**o treinamento**] e a precisão dos testes\n",
    "pode ser melhorado ainda mais: deixamos suas discussões nos exercícios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70669ab9",
   "metadata": {
    "id": "70669ab9",
    "origin_pos": 25,
    "outputId": "3f257283-0447-4bd2-8066-fcf8ed509945",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "net(next(iter(train_iter))[0])\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26196099",
   "metadata": {
    "id": "26196099",
    "origin_pos": 26
   },
   "source": [
    "## Resumo\n",
    "\n",
    "* Podemos ajustar o modelo BERT pré-treinado para aplicações posteriores, como inferência de linguagem natural no conjunto de dados SNLI.\n",
    "* Durante o ajuste fino, o modelo BERT se torna parte do modelo para o aplicativo downstream. Parâmetros que são relacionados apenas à perda de pré-treinamento não serão atualizados durante o ajuste fino.\n",
    "\n",
    "\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Ajuste fino de um modelo BERT pré-treinado muito maior que seja quase tão grande quanto o modelo base BERT original se seu recurso computacional permitir. Defina argumentos na função `load_pretrained_model` como: substituindo 'bert.small' por 'bert.base', aumentando os valores de `num_hiddens=256`, `ffn_num_hiddens=512`, `num_heads=4` e `num_blks=2` para 768, 3072, 12 e 12, respectivamente. Ao aumentar as épocas de ajuste fino (e possivelmente ajustar outros hiperparâmetros), você pode obter uma precisão de teste maior que 0,86?\n",
    "1. Como truncar um par de sequências de acordo com sua razão de comprimento? Compare este método de truncamento de pares e o usado na classe `SNLIBERTDataset`. Quais são seus prós e contras?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f408d0",
   "metadata": {
    "id": "e4f408d0",
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussões](https://discuss.d2l.ai/t/1526)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
