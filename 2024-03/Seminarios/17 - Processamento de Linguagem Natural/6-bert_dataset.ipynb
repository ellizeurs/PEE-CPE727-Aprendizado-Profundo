{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f267a5",
   "metadata": {
    "id": "03f267a5"
   },
   "source": [
    "As seguintes bibliotecas adicionais são necessárias para executar este\n",
    "notebook. Observe que a execução no Colab é experimental, por favor, relate um Github\n",
    "questão se você tiver algum problema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0615a",
   "metadata": {
    "id": "b3c0615a"
   },
   "outputs": [],
   "source": [
    "!pip install d2l==1.0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c03cda",
   "metadata": {
    "id": "14c03cda",
    "origin_pos": 0
   },
   "source": [
    "# O conjunto de dados para pré-treinamento BERT\n",
    ":label:`conjunto de dados sec_bert`\n",
    "\n",
    "Para pré-treinar o modelo BERT conforme implementado em :numref:`sec_bert`,\n",
    "precisamos gerar o conjunto de dados no formato ideal para facilitar\n",
    "as duas tarefas de pré-treinamento:\n",
    "modelagem de linguagem mascarada e previsão da próxima frase.\n",
    "Por um lado,\n",
    "o modelo BERT original é pré-treinado na concatenação de\n",
    "dois grandes corpora BookCorpus e Wikipedia em inglês (veja :numref:`subsec_bert_pretraining_tasks`),\n",
    "tornando difícil para a maioria dos leitores deste livro correr.\n",
    "Por outro lado,\n",
    "o modelo BERT pré-treinado pronto para uso\n",
    "pode não ser adequado para aplicações de domínios específicos, como medicina.\n",
    "Por isso, está se tornando popular pré-treinar o BERT em um conjunto de dados personalizado.\n",
    "Para facilitar a demonstração do pré-treinamento BERT,\n",
    "usamos um corpus menor WikiText-2 :cite:`Merity.Xiong.Bradbury.ea.2016`.\n",
    "\n",
    "Comparando com o conjunto de dados PTB usado para pré-treinamento word2vec em :numref:`sec_word2vec_data`,\n",
    "O WikiText-2 (i) mantém a pontuação original, tornando-o adequado para a previsão da próxima frase; (ii) mantém o caso e os números originais; (iii) é mais de duas vezes maior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b349c114",
   "metadata": {
    "id": "b349c114",
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00639076",
   "metadata": {
    "id": "00639076",
    "origin_pos": 3
   },
   "source": [
    "No [**conjunto de dados WikiText-2**],\n",
    "cada linha representa um parágrafo onde\n",
    "um espaço é inserido entre qualquer pontuação e seu token precedente.\n",
    "Parágrafos com pelo menos duas frases são mantidos.\n",
    "Para dividir frases, usamos apenas o ponto como delimitador para simplificar.\n",
    "Deixamos as discussões sobre técnicas mais complexas de divisão de frases nos exercícios\n",
    "no final desta seção.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea9beb6f",
   "metadata": {
    "id": "ea9beb6f",
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'http://la.ihainan.me/'\n",
    "    'wikitext-2-v1.zip', 'f6e734fc17885b364243f67b30385a3d')\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # Uppercase letters are converted to lowercase ones\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f3b50",
   "metadata": {
    "id": "686f3b50",
    "origin_pos": 5
   },
   "source": [
    "## Definindo funções auxiliares para tarefas de pré-treinamento\n",
    "\n",
    "A seguir,\n",
    "começamos implementando funções auxiliares para as duas tarefas de pré-treinamento de BERT:\n",
    "previsão da próxima frase e modelagem de linguagem mascarada.\n",
    "Essas funções auxiliares serão invocadas mais tarde\n",
    "ao transformar o corpus de texto bruto\n",
    "no conjunto de dados do formato ideal para pré-treinar o BERT.\n",
    "\n",
    "### [**Gerando a próxima tarefa de previsão de frases**]\n",
    "\n",
    "De acordo com as descrições de :numref:`subsec_nsp`,\n",
    "a função `_get_next_sentence` gera um exemplo de treinamento\n",
    "para a tarefa de classificação binária.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb7772df",
   "metadata": {
    "id": "bb7772df",
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # `paragraphs` is a list of lists of lists\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005849c",
   "metadata": {
    "id": "a005849c",
    "origin_pos": 7
   },
   "source": [
    "A função a seguir gera exemplos de treinamento para a previsão da próxima frase\n",
    "da entrada `paragraph` invocando a função `_get_next_sentence`.\n",
    "Aqui `parágrafo` é uma lista de frases, onde cada frase é uma lista de tokens.\n",
    "O argumento `max_len` especifica o comprimento máximo de uma sequência de entrada BERT durante o pré-treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a515271",
   "metadata": {
    "id": "8a515271",
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b9a12",
   "metadata": {
    "id": "074b9a12",
    "origin_pos": 9
   },
   "source": [
    "### [**Gerando a Tarefa de Modelagem de Linguagem Mascarada**]\n",
    ":label:`subsec_prepare_mlm_data`\n",
    "\n",
    "Para gerar exemplos de treinamento\n",
    "para a tarefa de modelagem de linguagem mascarada\n",
    "de uma sequência de entrada BERT,\n",
    "definimos a seguinte função `_replace_mlm_tokens`.\n",
    "Em suas entradas, `tokens` é uma lista de tokens que representam uma sequência de entrada BERT,\n",
    "`candidate_pred_positions` é uma lista de índices de token da sequência de entrada BERT\n",
    "excluindo aqueles de tokens especiais (tokens especiais não são previstos na tarefa de modelagem de linguagem mascarada),\n",
    "e `num_mlm_preds` indica o número de previsões (lembre-se de 15% de tokens aleatórios para prever).\n",
    "Seguindo a definição da tarefa de modelagem de linguagem mascarada em :numref:`subsec_mlm`,\n",
    "em cada posição de previsão, a entrada pode ser substituída por\n",
    "um token especial “&lt;mask&gt;” ou um token aleatório, ou permanecer inalterado.\n",
    "No final, a função retorna os tokens de entrada após possível substituição,\n",
    "os índices de token onde as previsões ocorrem e rótulos para essas previsões.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b4c14fe",
   "metadata": {
    "id": "5b4c14fe",
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
    "                        vocab):\n",
    "    # For the input of a masked language model, make a new copy of tokens and\n",
    "    # replace some of them by '<mask>' or random tokens\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    # Shuffle for getting 15% random tokens for prediction in the masked\n",
    "    # language modeling task\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80% of the time: replace the word with the '<mask>' token\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10% of the time: keep the word unchanged\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10% of the time: replace the word with a random word\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4dc18",
   "metadata": {
    "id": "9cc4dc18",
    "origin_pos": 11
   },
   "source": [
    "Ao invocar a função `_replace_mlm_tokens` mencionada anteriormente,\n",
    "a função a seguir recebe uma sequência de entrada BERT (`tokens`)\n",
    "como uma entrada e retorna índices dos tokens de entrada\n",
    "(após possível substituição de token conforme descrito em :numref:`subsec_mlm`),\n",
    "os índices de token onde as previsões ocorrem,\n",
    "e índices de rótulos para essas previsões.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c71c28c0",
   "metadata": {
    "id": "c71c28c0",
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    # `tokens` is a list of strings\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Special tokens are not predicted in the masked language modeling\n",
    "        # task\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 15% of random tokens are predicted in the masked language modeling task\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "                                       key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4716fe1",
   "metadata": {
    "id": "d4716fe1",
    "origin_pos": 13
   },
   "source": [
    "## Transformando texto no conjunto de dados de pré-treinamento\n",
    "\n",
    "Agora estamos quase prontos para personalizar uma classe `Dataset` para pré-treinamento de BERT.\n",
    "Antes disso,\n",
    "ainda precisamos definir uma função auxiliar `_pad_bert_inputs`\n",
    "para [**acrescentar os tokens especiais “&lt;pad&gt;” às entradas.**]\n",
    "Seu argumento `examples` contém as saídas das funções auxiliares `_get_nsp_data_from_paragraph` e `_get_mlm_data_from_tokens` para as duas tarefas de pré-treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2d2247",
   "metadata": {
    "id": "9b2d2247",
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
    "         is_next) in examples:\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
    "            max_len - len(token_ids)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (\n",
    "            max_len - len(segments)), dtype=torch.long))\n",
    "        # `valid_lens` excludes count of '<pad>' tokens\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
    "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # Predictions of padded tokens will be filtered out in the loss via\n",
    "        # multiplication of 0 weights\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
    "                max_num_mlm_preds - len(pred_positions)),\n",
    "                dtype=torch.float32))\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
    "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8f394",
   "metadata": {
    "id": "fdc8f394",
    "origin_pos": 16
   },
   "source": [
    "Colocando as funções auxiliares para\n",
    "gerando exemplos de treinamento das duas tarefas de pré-treinamento,\n",
    "e a função auxiliar para preencher entradas juntas,\n",
    "personalizamos a seguinte classe `_WikiTextDataset` como [**o conjunto de dados WikiText-2 para pré-treinamento de BERT**].\n",
    "Ao implementar a função `__getitem__ `,\n",
    "podemos acessar arbitrariamente os exemplos de pré-treinamento (modelagem de linguagem mascarada e previsão da próxima frase)\n",
    "gerado a partir de um par de frases do corpus WikiText-2.\n",
    "\n",
    "O modelo BERT original usa embeddings WordPiece cujo tamanho de vocabulário é 30000 :cite:`Wu.Schuster.Chen.ea.2016`.\n",
    "O método de tokenização do WordPiece é uma ligeira modificação do\n",
    "o algoritmo de codificação de pares de bytes original em :numref:`subsec_Byte_Pair_Encoding`.\n",
    "Para simplificar, usamos a função `d2l.tokenize` para tokenização.\n",
    "Tokens pouco frequentes que aparecem menos de cinco vezes são filtrados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27a00351",
   "metadata": {
    "id": "27a00351",
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # Input `paragraphs[i]` is a list of sentence strings representing a\n",
    "        # paragraph; while output `paragraphs[i]` is a list of sentences\n",
    "        # representing a paragraph, where each sentence is a list of tokens\n",
    "        paragraphs = [d2l.tokenize(\n",
    "            paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs\n",
    "                     for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # Get data for the next sentence prediction task\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        # Get data for the masked language model task\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                      + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        # Pad inputs\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
    "            examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d11c15",
   "metadata": {
    "id": "16d11c15",
    "origin_pos": 19
   },
   "source": [
    "Ao usar a função `_read_wiki` e a classe `_WikiTextDataset`,\n",
    "definimos o seguinte `load_data_wiki` para [**baixar e conjunto de dados WikiText-2\n",
    "e gerar exemplos de pré-treinamento**] a partir dele.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8efaa8",
   "metadata": {
    "id": "6e8efaa8",
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"Load the WikiText-2 dataset.\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                        shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2986f60",
   "metadata": {
    "id": "d2986f60",
    "origin_pos": 22
   },
   "source": [
    "Definir o tamanho do lote como 512 e o comprimento máximo de uma sequência de entrada BERT como 64,\n",
    "nós [**imprimimos as formas de um minilote de exemplos de pré-treinamento BERT**].\n",
    "Observe que em cada sequência de entrada BERT,\n",
    "$10$ ($64 \\vezes 0,15$) posições são previstas para a tarefa de modelagem de linguagem mascarada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b866b9c",
   "metadata": {
    "id": "6b866b9c",
    "origin_pos": 23,
    "outputId": "98b42515-8724-4626-84fa-d2edceaa58e8",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/wikitext-2-v1.zip from http://la.ihainan.me/wikitext-2-v1.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellizeu.sena/Documentos/Aprendizado Profundo/Processamento de Linguagem Natural/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
    "     mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9fd56",
   "metadata": {
    "id": "e5d9fd56",
    "origin_pos": 24
   },
   "source": [
    "No final, vamos dar uma olhada no tamanho do vocabulário.\n",
    "Mesmo depois de filtrar tokens pouco frequentes,\n",
    "ainda é mais de duas vezes maior que o conjunto de dados PTB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76cf9172",
   "metadata": {
    "id": "76cf9172",
    "origin_pos": 25,
    "outputId": "72a4644d-c2c7-42a3-e4bb-eb74d7bb8fa2",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b598fc5",
   "metadata": {
    "id": "3b598fc5",
    "origin_pos": 26
   },
   "source": [
    "## Resumo\n",
    "\n",
    "* Comparado com o conjunto de dados PTB, o conjunto de dados WikiText-2 mantém a pontuação, caixa e números originais e é mais de duas vezes maior.\n",
    "* Podemos acessar arbitrariamente os exemplos de pré-treinamento (modelagem de linguagem mascarada e previsão da próxima frase) gerados a partir de um par de frases do corpus WikiText-2.\n",
    "\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Para simplificar, o ponto é usado como o único delimitador para dividir sentenças. Tente outras técnicas de divisão de sentenças, como spaCy e NLTK. Tome o NLTK como exemplo. Você precisa instalar o NLTK primeiro: `pip install nltk`. No código, primeiro `import nltk`. Em seguida, baixe o tokenizador de sentenças Punkt: `nltk.download('punkt')`. Para dividir sentenças como `sentences = 'This is great ! Why not ?'`, invocar `nltk.tokenize.sent_tokenize(sentences)` retornará uma lista de duas strings de sentenças: `['This is great !', 'Why not ?']`.\n",
    "1. Qual é o tamanho do vocabulário se não filtrarmos nenhum token infrequente?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad35f4a",
   "metadata": {
    "id": "aad35f4a",
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussões](https://discuss.d2l.ai/t/1496)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
